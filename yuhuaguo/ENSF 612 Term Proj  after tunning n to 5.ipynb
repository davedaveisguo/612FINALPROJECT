{"cells":[{"cell_type":"markdown","source":["####Install"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38f3da1f-2c54-4b25-93d1-c59410fc56a8"}}},{"cell_type":"code","source":["%sh\npip install nltk\npip install stop-words\npip install pyspellchecker"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6b19a94-3f49-46d3-85f1-24c9040d89c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Requirement already satisfied: nltk in /databricks/python3/lib/python3.8/site-packages (3.6.1)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (from nltk) (4.59.0)\nRequirement already satisfied: regex in /databricks/python3/lib/python3.8/site-packages (from nltk) (2021.4.4)\nRequirement already satisfied: click in /databricks/python3/lib/python3.8/site-packages (from nltk) (7.1.2)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (1.0.1)\nWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\nCollecting stop-words\n  Downloading stop-words-2018.7.23.tar.gz (31 kB)\nBuilding wheels for collected packages: stop-words\n  Building wheel for stop-words (setup.py): started\n  Building wheel for stop-words (setup.py): finished with status 'done'\n  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32916 sha256=2f2d9d9e4413a34c0df26834157e74c59d3e21f565a357acfe8217009eb63217\n  Stored in directory: /root/.cache/pip/wheels/eb/03/0d/3bd31c983789aeb0b4d5e2ca48590288d9db1586cf5f225062\nSuccessfully built stop-words\nInstalling collected packages: stop-words\nSuccessfully installed stop-words-2018.7.23\nWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\nCollecting pyspellchecker\n  Downloading pyspellchecker-0.6.2-py3-none-any.whl (2.7 MB)\nInstalling collected packages: pyspellchecker\nSuccessfully installed pyspellchecker-0.6.2\nWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: nltk in /databricks/python3/lib/python3.8/site-packages (3.6.1)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (from nltk) (4.59.0)\nRequirement already satisfied: regex in /databricks/python3/lib/python3.8/site-packages (from nltk) (2021.4.4)\nRequirement already satisfied: click in /databricks/python3/lib/python3.8/site-packages (from nltk) (7.1.2)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (1.0.1)\nWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\nCollecting stop-words\n  Downloading stop-words-2018.7.23.tar.gz (31 kB)\nBuilding wheels for collected packages: stop-words\n  Building wheel for stop-words (setup.py): started\n  Building wheel for stop-words (setup.py): finished with status 'done'\n  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32916 sha256=2f2d9d9e4413a34c0df26834157e74c59d3e21f565a357acfe8217009eb63217\n  Stored in directory: /root/.cache/pip/wheels/eb/03/0d/3bd31c983789aeb0b4d5e2ca48590288d9db1586cf5f225062\nSuccessfully built stop-words\nInstalling collected packages: stop-words\nSuccessfully installed stop-words-2018.7.23\nWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\nCollecting pyspellchecker\n  Downloading pyspellchecker-0.6.2-py3-none-any.whl (2.7 MB)\nInstalling collected packages: pyspellchecker\nSuccessfully installed pyspellchecker-0.6.2\nWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["####TBD\n\n1. Tokenization into words\n2. Stop words removal\n3. Noise reduction (e.g., removal of punctuation)\n4. Stemmin"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2607ecf6-bb01-4aa9-8f1c-713d0b6cb8d0"}}},{"cell_type":"markdown","source":["#### 1. Load Data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fef7f24-305e-426a-b827-6ec91dca263a"}}},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\n# File location and type\nfile_location = \"/FileStore/tables/additional.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\ndf = spark.read.format(file_type).option(\"inferSchema\", infer_schema).option(\"header\", \"true\").option(\"sep\", delimiter).load(file_location)\n\n\npandasDF_news = df.select('news').toPandas()\npandasDF_target = df.select('target').toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"065a8ba3-3692-4147-80a6-ca891b28cda9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from sklearn.datasets import fetch_20newsgroups\nimport pandas as pd\nimport numpy as np\nimport re\nfrom pyspark.sql import SQLContext\n\ncategories = ['rec.autos', 'rec.sport.baseball', 'comp.graphics', 'comp.sys.mac.hardware', \n              'sci.space', 'sci.crypt', 'talk.politics.guns', 'talk.religion.misc']\nnewsgroup = fetch_20newsgroups(subset='train',categories= categories , shuffle=True, random_state=42)\n\ndf_news = pd.DataFrame(data=newsgroup.data, columns=['news']) \n\ndf_news = df_news.append(pandasDF_news, ignore_index=True)\n\ndf_news = df_news.replace(re.compile(r\"From: \\S*@\\S*\\s?\"),\"\")\ndf_news = df_news.replace(re.compile('\\s+'),\" \")\ndf_news = df_news.replace(re.compile(\"\\'\"),\"\")\n\n#df_news = df_news.dropna()\n\ndf_target = pd.DataFrame(data=newsgroup.target, columns=['target'])\n\ndf_target = df_target.append(pandasDF_target, ignore_index=True)\n#df_target = df_target.dropna()\n\ndf_target['target']=df_target.target.astype('int64')\n\ndf_binary_labels = pd.DataFrame(np.where (df_target < 10, 0, 1), columns=['Binary Label'])\n\nsqlContext = SQLContext(sc)\ndf_newsgroup = sqlContext.createDataFrame(pd.concat([df_news, df_target, df_binary_labels], axis=1))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76b7b165-7c1a-4f34-a415-2234591295ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/databricks/spark/python/pyspark/sql/context.py:82: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/databricks/spark/python/pyspark/sql/context.py:82: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### 2. Pipeline"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5406831-fd9c-410b-9017-0d161c64d6ea"}}},{"cell_type":"code","source":["from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\nfrom pyspark.ml.feature import Tokenizer, HashingTF, IDF, StringIndexer\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.classification import LinearSVC\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\n\nregexTokenizer = RegexTokenizer(inputCol=\"news\", outputCol=\"news_words\", pattern=\"\\\\W\")\nadd_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\",\"subject\",\"re\",'.',',','', 'i i','?','\\'\\'',\"''\",'y','*','out','==','df','e.g.','\\'m','\\[',\"'m\",':', ')', '(','n\\'t', '\\'','``','``','\\'s', 'https://','-'] \nstopwordsRemover = StopWordsRemover(inputCol=\"news_words\", outputCol=\"filtered\").setStopWords(add_stopwords)\nhashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\nstring_indexer = StringIndexer(inputCol = \"target\", outputCol = \"target_indexed\")\n\npipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, string_indexer])\npipelineFit = pipeline.fit(df_newsgroup)\n\ndataset = pipelineFit.transform(df_newsgroup)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n#lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\nrf = RandomForestClassifier(featuresCol=idf.getOutputCol(), labelCol=string_indexer.getOutputCol(), maxDepth=10)\nrf_mod = rf.fit(trainingData)\n#lrModel = lr.fit(trainingData)\n#predictions = lrModel.transform(testData)\npredictions = rf_mod.transform(testData)\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"target_indexed\", predictionCol=\"prediction\")\nevaluator.evaluate(predictions)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad0fc6b2-eb39-4fb5-a34b-5b95ab291724"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[9]: 0.7032946941465295","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[9]: 0.7032946941465295"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### 3. Evaluate ML Model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ceb0b67-d451-492d-8e4b-158003aa5a9a"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"target_indexed\", predictionCol=\"prediction\")\nevaluator.evaluate(predictions)\n\naccuracy = evaluator.evaluate(predictions)\nprint(\"Accuracy = %s\" % (accuracy))\nprint(\"Test Error = %s\" % (1.0 - accuracy))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41257130-38d7-4cfa-93cf-9817087f8e6f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Accuracy = 0.7032946941465295\nTest Error = 0.29670530585347055\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Accuracy = 0.7032946941465295\nTest Error = 0.29670530585347055\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### 4. Parameter tuning"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b5076f5-04d6-456b-bb50-e8c42f26d850"}}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder\nfrom pyspark.ml.tuning import CrossValidator\n\ntrainingData1 = trainingData.drop(\"news_words\",\"news_tf\",\"news_tfidf\",\"rawPrediction\",\"probability\",\"prediction\",\"filtered\",\"rawFeatures\",\"CrossValidator_2b30ebf36fbb_rand\")\ntestData1 = testData.drop(\"news_words\",\"news_tf\",\"news_tfidf\",\"rawPrediction\",\"probability\",\"prediction\",\"filtered\",\"rawFeatures\",\"CrossValidator_2b30ebf36fbb_rand\")\n\n\ntrainingData1.show(5)\n\n#grid for randomforest\ngrid = (ParamGridBuilder().baseOn([evaluator.metricName, 'precision']).addGrid(rf.maxDepth, [10, 20]).build())\n\n\n# Instanciation of a CrossValidator\ncv = CrossValidator(estimator=rf, estimatorParamMaps=grid, evaluator=evaluator, numFolds=3)\n\n# Transform the data and train the classifier on the training set\ncv_model = cv.fit(trainingData1)\n\n# Transform the data and perform predictions on the test set\ndf_test_pred1 = cv_model.transform(testData1)\n\n# Evaluate the predictions done on the test set\nevaluator.evaluate(df_test_pred1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ab37513-45ba-49c6-be34-5229cf7281fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+------+------------+--------------------+--------------+\n|                news|target|Binary Label|            features|target_indexed|\n+--------------------+------+------------+--------------------+--------------+\n| (Peter van der V...|     0|           0|(10000,[42,66,120...|           5.0|\n|( Nikan B Firoozy...|     5|           0|(10000,[55,222,26...|           4.0|\n|( Phil Mueller ) ...|     2|           0|(10000,[15,78,207...|           1.0|\n|(\"Imaging Club\") ...|     0|           0|(10000,[78,452,48...|           5.0|\n|(\"RWTMS2::MUNIZB\"...|     5|           0|(10000,[66,78,86,...|           4.0|\n+--------------------+------+------------+--------------------+--------------+\nonly showing top 5 rows\n\nDeprecation warning: The PySpark MLlib + MLflow Tracking integration is deprecated. Soon, we will remove this feature and replace it with MLflow PySpark ML autologging and Databricks Autologging.\n To try the new MLflow PySpark ML autologging feature, which will be enabled by default in an upcoming release, call `mlflow.pyspark.ml.autolog()`.\nMLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+------+------------+--------------------+--------------+\n|                news|target|Binary Label|            features|target_indexed|\n+--------------------+------+------------+--------------------+--------------+\n| (Peter van der V...|     0|           0|(10000,[42,66,120...|           5.0|\n|( Nikan B Firoozy...|     5|           0|(10000,[55,222,26...|           4.0|\n|( Phil Mueller ) ...|     2|           0|(10000,[15,78,207...|           1.0|\n|(\"Imaging Club\") ...|     0|           0|(10000,[78,452,48...|           5.0|\n|(\"RWTMS2::MUNIZB\"...|     5|           0|(10000,[66,78,86,...|           4.0|\n+--------------------+------+------------+--------------------+--------------+\nonly showing top 5 rows\n\nDeprecation warning: The PySpark MLlib + MLflow Tracking integration is deprecated. Soon, we will remove this feature and replace it with MLflow PySpark ML autologging and Databricks Autologging.\n To try the new MLflow PySpark ML autologging feature, which will be enabled by default in an upcoming release, call `mlflow.pyspark.ml.autolog()`.\nMLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\n"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ENSF 612 Term Proj  RF 70","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3293546080896689}},"nbformat":4,"nbformat_minor":0}
